# ==============================================================================
# Part 1: Configuration
# ==============================================================================

# Default weights for the three-category model.
# These represent the relative importance of each category.
DEFAULT_WEIGHTS = {
    "material_composition": 4,
    "production_and_brand": 4,
    "circularity_and_end_of_life": 2,
}

# A single, standardized map for converting the LLM's ratings into numerical scores.
# The scale is from -1.0 (very bad) to 1.0 (very good).
RATING_SCORES = {
    'Excellent': 10,
    'Good': 8,
    'Neutral': 5,
    'Poor': 0,
    'Unknown': -1, # Special case for unknown
}

import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('scorer')

# ==============================================================================
# Part 2: Scorer Functions
# ==============================================================================

def generate_sustainability_breakdown(analysis_json: dict) -> dict:
    """
    Extracts the new three-category breakdown structure from the LLM's analysis.
    This creates the rich object used for both display and calculation.

    Args:
        analysis_json: The full structured JSON object from the analyzer.

    Returns:
        A dictionary containing the detailed sustainability breakdown.
    """
    logger.info(f"Input to generate_sustainability_breakdown: {json.dumps(analysis_json, indent=2)}")
    breakdown = {}
    # The new analysis is nested under the 'sustainability_analysis' key
    sustainability_analysis = analysis_json.get('sustainability_analysis', {})
    logger.info(f"Extracted sustainability_analysis: {json.dumps(sustainability_analysis, indent=2)}")
    # Iterate through our three main categories
    for category, details in sustainability_analysis.items():
        rating = details.get('rating', 'Unknown')
        logger.debug(f"Processing category: {category}, rating: {rating}")
        breakdown[category] = {
            "value": rating,  # The qualitative rating (e.g., "Good")
            "score": RATING_SCORES.get(rating, 0.0), # The quantitative score
            "analysis": details.get('analysis', 'No analysis provided.')
        }
    logger.info(f"Generated breakdown: {json.dumps(breakdown, indent=2)}")
    return breakdown


def calculate_weighted_score(sustainability_breakdown: dict, user_weights: dict | None = None) -> int:
    """
    Calculates the final 0-100 score from the breakdown object and weights.
    This is a direct, unadjusted calculation.

    Args:
        sustainability_breakdown: The object generated by generate_sustainability_breakdown().
        user_weights: The user's personalized weights. If None, uses default weights.

    Returns:
        The final sustainability score, an integer between 0 and 100.
    """
    # Use user-provided weights, or fall back to the defaults
    weights = user_weights or DEFAULT_WEIGHTS
    total_weighted_score = 0
    # Iterate through the breakdown object to calculate the total score
    for category, breakdown_details in sustainability_breakdown.items():
        # Map new 0-10 scale to old -1.0 to 1.0 scale for scoring logic
        score = breakdown_details.get('score', -1)
        if score == -1:
            normalized_score = 0.0 # treat unknown as neutral for overall
        else:
            normalized_score = (score - 5) / 5  # 0->-1, 5->0, 10->1
        weight = weights.get(category, 0)
        total_weighted_score += normalized_score * weight

    # Normalize the final score to be on a 0-100 scale
    total_weights = sum(weights.values())
    if total_weights == 0:
        return 50 # Return a neutral 50 if weights are all zero

    # The formula maps the raw score range [ -total_weights, +total_weights ] to [ 0, 100 ]
    normalized_score = 50 + 50 * (total_weighted_score / total_weights)
    
    # Clamp the result to ensure it's always within the 0-100 range
    return max(0, min(100, int(normalized_score)))